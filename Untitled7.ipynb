{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gojo-satoru7778/Roronoa_Zoro.github.io/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAFQEii0n0C8",
        "outputId": "920bfb6d-95aa-454a-e739-39c5bb7b2b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 STARTING COMPLETE ENHANCED ACOUSTIC FAULT DETECTION ANALYSIS\n",
            "================================================================================\n",
            "🔄 Generating synthetic datasets...\n",
            "✅ Generated Engine_01: 168 samples\n",
            "✅ Generated Engine_02: 151 samples\n",
            "✅ Generated Engine_03: 166 samples\n",
            "✅ Generated Engine_04: 138 samples\n",
            "✅ Generated Engine_05: 150 samples\n",
            "✅ Generated Engine_06: 175 samples\n",
            "✅ Generated Engine_07: 169 samples\n",
            "✅ Generated Engine_08: 179 samples\n",
            "✅ Generated Engine_09: 167 samples\n",
            "✅ Generated Engine_10: 178 samples\n",
            "✅ Generated Engine_11: 157 samples\n",
            "✅ Generated Engine_12: 144 samples\n",
            "✅ Generated Engine_13: 149 samples\n",
            "✅ Generated Engine_14: 135 samples\n",
            "✅ Generated Engine_15: 138 samples\n",
            "✅ Generated Engine_16: 169 samples\n",
            "✅ Generated Engine_17: 136 samples\n",
            "✅ Generated Engine_18: 139 samples\n",
            "✅ Generated Engine_19: 151 samples\n",
            "✅ Generated Engine_20: 147 samples\n",
            "✅ Generated Engine_21: 139 samples\n",
            "✅ Generated Engine_22: 175 samples\n",
            "✅ Generated Engine_23: 156 samples\n",
            "✅ Generated Engine_24: 136 samples\n",
            "✅ Generated Engine_25: 132 samples\n",
            "✅ Generated Engine_26: 163 samples\n",
            "✅ Generated Engine_27: 161 samples\n",
            "✅ Generated Engine_28: 136 samples\n",
            "✅ Generated Engine_29: 178 samples\n",
            "✅ Generated Engine_30: 174 samples\n",
            "✅ Generated Engine_31: 144 samples\n",
            "✅ Generated Engine_32: 174 samples\n",
            "✅ Generated Engine_33: 134 samples\n",
            "✅ Generated Engine_34: 138 samples\n",
            "✅ Generated Engine_35: 179 samples\n",
            "✅ Generated Engine_36: 152 samples\n",
            "✅ Generated Engine_37: 176 samples\n",
            "✅ Generated Engine_38: 143 samples\n",
            "✅ Generated Engine_39: 130 samples\n",
            "✅ Generated Engine_40: 156 samples\n",
            "✅ Generated Engine_41: 151 samples\n",
            "✅ Generated Engine_42: 137 samples\n",
            "✅ Generated Engine_43: 142 samples\n",
            "✅ Generated Engine_44: 174 samples\n",
            "✅ Generated Engine_45: 130 samples\n",
            "✅ Generated Engine_46: 157 samples\n",
            "✅ Generated Engine_47: 174 samples\n",
            "✅ Generated Engine_48: 164 samples\n",
            "✅ Generated Engine_49: 135 samples\n",
            "✅ Generated Engine_50: 163 samples\n",
            "✅ Generated Engine_51: 171 samples\n",
            "✅ Generated Engine_52: 142 samples\n",
            "✅ Generated Engine_53: 137 samples\n",
            "✅ Generated Engine_54: 170 samples\n",
            "✅ Generated Engine_55: 150 samples\n",
            "✅ Generated Engine_56: 134 samples\n",
            "✅ Generated Engine_57: 136 samples\n",
            "✅ Generated Engine_58: 159 samples\n",
            "✅ Generated Engine_59: 148 samples\n",
            "✅ Generated Engine_60: 162 samples\n",
            "✅ Generated Engine_61: 171 samples\n",
            "✅ Generated Engine_62: 132 samples\n",
            "✅ Generated Engine_63: 171 samples\n",
            "✅ Generated Engine_64: 137 samples\n",
            "✅ Generated Engine_65: 161 samples\n",
            "✅ Generated Engine_66: 173 samples\n",
            "✅ Generated Engine_67: 176 samples\n",
            "✅ Generated Engine_68: 131 samples\n",
            "✅ Generated Engine_69: 163 samples\n",
            "✅ Generated Engine_70: 173 samples\n",
            "✅ Generated Engine_71: 130 samples\n",
            "✅ Generated Engine_72: 137 samples\n",
            "✅ Generated Engine_73: 161 samples\n",
            "✅ Generated Engine_74: 140 samples\n",
            "✅ Generated Engine_75: 149 samples\n",
            "✅ Generated Engine_76: 165 samples\n",
            "✅ Generated Engine_77: 157 samples\n",
            "✅ Generated Engine_78: 162 samples\n",
            "✅ Generated Engine_79: 150 samples\n",
            "✅ Generated Engine_80: 170 samples\n",
            "✅ Generated Engine_81: 154 samples\n",
            "✅ Generated Engine_82: 139 samples\n",
            "✅ Generated Engine_83: 148 samples\n",
            "✅ Generated Engine_84: 139 samples\n",
            "✅ Generated Engine_85: 142 samples\n",
            "✅ Generated Engine_86: 133 samples\n",
            "✅ Generated Engine_87: 161 samples\n",
            "✅ Generated Engine_88: 178 samples\n",
            "✅ Generated Engine_89: 145 samples\n",
            "✅ Generated Engine_90: 142 samples\n",
            "✅ Generated Engine_91: 146 samples\n",
            "✅ Generated Engine_92: 170 samples\n",
            "✅ Generated Engine_93: 130 samples\n",
            "✅ Generated Engine_94: 157 samples\n",
            "✅ Generated Engine_95: 147 samples\n",
            "✅ Generated Engine_96: 144 samples\n",
            "✅ Generated Engine_97: 137 samples\n",
            "✅ Generated Engine_98: 157 samples\n",
            "✅ Generated Engine_99: 151 samples\n",
            "✅ Generated Engine_100: 156 samples\n",
            "📊 Total datasets: 100\n",
            "\n",
            "🤖 Creating and Training Machine Learning Models...\n",
            "================================================================================\n",
            "📊 Dataset Info:\n",
            "   Total samples: 15303\n",
            "   Features: 10\n",
            "   Target range: 0.000 - 1.000\n",
            "   Training samples: 12242\n",
            "   Testing samples: 3061\n",
            "\n",
            "============================================================\n",
            "🔄 Training Linear Regression...\n",
            "\n",
            "🔧 Linear Regression Parameters:\n",
            "--------------------------------------------------\n",
            "Coefficients: [ 1.36571360e-04  2.54279088e-01  2.15752186e-03  2.77484449e-05\n",
            "  1.52981185e+00 -9.94034432e-05 -9.49582473e-03 -1.89696526e-02\n",
            "  7.04902126e-02  1.33442853e-02]\n",
            "Intercept: -0.04548480491173368\n",
            "\n",
            "📈 Performance Metrics:\n",
            "   Training R²: 0.1511\n",
            "   Training MSE: 0.0918\n",
            "   Test R²: 0.1486\n",
            "   Test MSE: 0.0916\n",
            "   Test MAE: 0.2540\n",
            "   CV R² (mean ± std): 0.1496 ± 0.0058\n",
            "   ✅ Good generalization\n",
            "\n",
            "============================================================\n",
            "🔄 Training Ridge Regression...\n",
            "\n",
            "🔧 Ridge Regression Parameters:\n",
            "--------------------------------------------------\n",
            "Coefficients: [ 0.09579094  0.06382856  0.0586762   0.03593187  0.06192611 -0.07228144\n",
            " -0.00191273 -0.00188425  0.00707779  0.00134141]\n",
            "Intercept: 0.5499639488304192\n",
            "Alpha: 1.0\n",
            "\n",
            "📈 Performance Metrics:\n",
            "   Training R²: 0.1511\n",
            "   Training MSE: 0.0918\n",
            "   Test R²: 0.1486\n",
            "   Test MSE: 0.0915\n",
            "   Test MAE: 0.2540\n",
            "   CV R² (mean ± std): 0.1496 ± 0.0058\n",
            "   ✅ Good generalization\n",
            "\n",
            "============================================================\n",
            "🔄 Training Lasso Regression...\n",
            "\n",
            "🔧 Lasso Regression Parameters:\n",
            "--------------------------------------------------\n",
            "Coefficients: [ 0.  0.  0.  0.  0.  0. -0. -0.  0.  0.]\n",
            "Intercept: 0.5499639488304192\n",
            "Alpha: 0.1\n",
            "\n",
            "📈 Performance Metrics:\n",
            "   Training R²: 0.0000\n",
            "   Training MSE: 0.1081\n",
            "   Test R²: -0.0000\n",
            "   Test MSE: 0.1075\n",
            "   Test MAE: 0.2834\n",
            "   CV R² (mean ± std): -0.0002 ± 0.0001\n",
            "   ✅ Good generalization\n",
            "\n",
            "============================================================\n",
            "🔄 Training Random Forest...\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Enhanced Acoustic Fault Detection in Vehicle Engines Using Machine Learning\n",
        "Author: ML Engineering Team\n",
        "Date: 2025\n",
        "\n",
        "This system uses machine learning to detect faults in vehicle engines\n",
        "based on acoustic signals and engine parameters with enhanced visualization\n",
        "and model parameter analysis.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "class EnhancedAcousticFaultDetection:\n",
        "    \"\"\"Enhanced class for acoustic fault detection system with detailed analysis\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.datasets = {}\n",
        "        self.models = {}\n",
        "        self.scaler = StandardScaler()\n",
        "        self.is_trained = False\n",
        "        self.best_model = None\n",
        "        self.best_model_name = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.X_train_scaled = None\n",
        "        self.X_test_scaled = None\n",
        "\n",
        "    def generate_datasets(self, num_datasets=100):\n",
        "        \"\"\"Generate synthetic datasets representing different engine conditions\"\"\"\n",
        "        print(\"🔄 Generating synthetic datasets...\")\n",
        "\n",
        "        for i in range(num_datasets):\n",
        "            dataset_name = f\"Engine_{i+1:02d}\"\n",
        "\n",
        "            # Different engine conditions for each dataset\n",
        "            conditions = {\n",
        "                'base_frequency': 800 + i * 20, # Adjusted scaling for 100 datasets\n",
        "                'noise_level': 0.1 + i * 0.005, # Adjusted scaling for 100 datasets\n",
        "                'temperature_bias': 60 + i * 0.8, # Adjusted scaling for 100 datasets\n",
        "                'rpm_range': (800 + i * 10, 6000 - i * 5) # Adjusted scaling for 100 datasets\n",
        "            }\n",
        "\n",
        "            data = self._generate_synthetic_data(\n",
        "                size=150 + np.random.randint(-20, 30),\n",
        "                conditions=conditions\n",
        "            )\n",
        "\n",
        "            self.datasets[dataset_name] = data\n",
        "            print(f\"✅ Generated {dataset_name}: {len(data)} samples\")\n",
        "\n",
        "        print(f\"📊 Total datasets: {len(self.datasets)}\")\n",
        "        return self.datasets\n",
        "\n",
        "    def _generate_synthetic_data(self, size, conditions):\n",
        "        \"\"\"Generate synthetic acoustic data for engine fault detection\"\"\"\n",
        "        data = []\n",
        "\n",
        "        for _ in range(size):\n",
        "            # Base parameters with condition-specific variations\n",
        "            frequency = conditions['base_frequency'] + np.random.normal(0, 400)\n",
        "            frequency = np.clip(frequency, 500, 4000)\n",
        "\n",
        "            amplitude = 0.1 + np.random.exponential(0.3)\n",
        "            amplitude = np.clip(amplitude, 0.05, 1.0)\n",
        "\n",
        "            temperature = conditions['temperature_bias'] + np.random.normal(0, 15)\n",
        "            temperature = np.clip(temperature, 40, 150)\n",
        "\n",
        "            rpm_min, rpm_max = conditions['rpm_range']\n",
        "            rpm = np.random.uniform(rpm_min, rpm_max)\n",
        "\n",
        "            harmonic_distortion = np.random.uniform(0.01, 0.15)\n",
        "            spectral_centroid = frequency * (1 + np.random.normal(0, 0.1))\n",
        "            zero_crossing_rate = np.random.uniform(0.1, 0.8)\n",
        "\n",
        "            vibration_x = np.random.normal(0, 0.1)\n",
        "            vibration_y = np.random.normal(0, 0.1)\n",
        "            vibration_z = np.random.normal(0, 0.1)\n",
        "\n",
        "            fault_score = self._calculate_fault_score(\n",
        "                frequency, amplitude, temperature, rpm,\n",
        "                harmonic_distortion, spectral_centroid, zero_crossing_rate,\n",
        "                vibration_x, vibration_y, vibration_z\n",
        "            )\n",
        "\n",
        "            fault_score += np.random.normal(0, conditions['noise_level'])\n",
        "            fault_score = np.clip(fault_score, 0, 1)\n",
        "\n",
        "            data.append({\n",
        "                'frequency': frequency,\n",
        "                'amplitude': amplitude,\n",
        "                'temperature': temperature,\n",
        "                'rpm': rpm,\n",
        "                'harmonic_distortion': harmonic_distortion,\n",
        "                'spectral_centroid': spectral_centroid,\n",
        "                'zero_crossing_rate': zero_crossing_rate,\n",
        "                'vibration_x': vibration_x,\n",
        "                'vibration_y': vibration_y,\n",
        "                'vibration_z': vibration_z,\n",
        "                'fault_score': fault_score\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _calculate_fault_score(self, freq, amp, temp, rpm, harm_dist,\n",
        "                              spec_cent, zcr, vib_x, vib_y, vib_z):\n",
        "        \"\"\"Calculate fault score based on multiple acoustic and engine parameters\"\"\"\n",
        "        score = 0\n",
        "\n",
        "        if freq > 2500:\n",
        "            score += 0.25\n",
        "        if freq > 3200:\n",
        "            score += 0.15\n",
        "        if freq < 1000:\n",
        "            score += 0.1\n",
        "\n",
        "        if amp > 0.6:\n",
        "            score += 0.2\n",
        "        if amp > 0.8:\n",
        "            score += 0.15\n",
        "\n",
        "        if temp > 100:\n",
        "            score += 0.15\n",
        "        if temp > 120:\n",
        "            score += 0.2\n",
        "        if temp < 60:\n",
        "            score += 0.1\n",
        "\n",
        "        if rpm > 4500:\n",
        "            score += 0.1\n",
        "        if rpm > 5500:\n",
        "            score += 0.15\n",
        "\n",
        "        score += harm_dist * 2\n",
        "\n",
        "        expected_centroid = freq * 1.2\n",
        "        centroid_deviation = abs(spec_cent - expected_centroid) / expected_centroid\n",
        "        score += centroid_deviation * 0.3\n",
        "\n",
        "        if zcr < 0.2 or zcr > 0.7:\n",
        "            score += 0.15\n",
        "\n",
        "        total_vibration = np.sqrt(vib_x**2 + vib_y**2 + vib_z**2)\n",
        "        if total_vibration > 0.2:\n",
        "            score += 0.25\n",
        "\n",
        "        return np.clip(score, 0, 1)\n",
        "\n",
        "    def combine_datasets(self):\n",
        "        \"\"\"Combine all datasets for training\"\"\"\n",
        "        if not self.datasets:\n",
        "            raise ValueError(\"No datasets available. Generate datasets first.\")\n",
        "\n",
        "        combined_data = []\n",
        "        for name, df in self.datasets.items():\n",
        "            df_copy = df.copy()\n",
        "            df_copy['dataset'] = name\n",
        "            combined_data.append(df_copy)\n",
        "\n",
        "        return pd.concat(combined_data, ignore_index=True)\n",
        "\n",
        "    def print_model_parameters(self, model, model_name):\n",
        "        \"\"\"Print detailed model parameters\"\"\"\n",
        "        print(f\"\\n🔧 {model_name} Parameters:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        try:\n",
        "            if hasattr(model, 'coef_'):\n",
        "                print(f\"Coefficients: {model.coef_}\")\n",
        "            if hasattr(model, 'intercept_'):\n",
        "                print(f\"Intercept: {model.intercept_}\")\n",
        "            if hasattr(model, 'alpha'):\n",
        "                print(f\"Alpha: {model.alpha}\")\n",
        "            if hasattr(model, 'n_estimators'):\n",
        "                print(f\"Number of estimators: {model.n_estimators}\")\n",
        "            if hasattr(model, 'max_depth'):\n",
        "                print(f\"Max depth: {model.max_depth}\")\n",
        "            if hasattr(model, 'C'):\n",
        "                print(f\"C parameter: {model.C}\")\n",
        "            if hasattr(model, 'gamma'):\n",
        "                print(f\"Gamma: {model.gamma}\")\n",
        "            if hasattr(model, 'kernel'):\n",
        "                print(f\"Kernel: {model.kernel}\")\n",
        "            if hasattr(model, 'learning_rate'):\n",
        "                print(f\"Learning rate: {model.learning_rate}\")\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                print(f\"Feature importances available: Yes\")\n",
        "                feature_names = [\n",
        "                    'Frequency', 'Amplitude', 'Temperature', 'RPM',\n",
        "                    'Harmonic Dist.', 'Spectral Cent.', 'Zero Cross Rate',\n",
        "                    'Vibration X', 'Vibration Y', 'Vibration Z'\n",
        "                ]\n",
        "                for i, importance in enumerate(model.feature_importances_):\n",
        "                    print(f\"  {feature_names[i]}: {importance:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error accessing parameters: {e}\")\n",
        "\n",
        "    def create_and_train_models(self):\n",
        "        \"\"\"Create and train multiple machine learning models with detailed output\"\"\"\n",
        "        print(\"\\n🤖 Creating and Training Machine Learning Models...\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Combine all datasets\n",
        "        data = self.combine_datasets()\n",
        "\n",
        "        # Prepare features and target\n",
        "        feature_columns = [\n",
        "            'frequency', 'amplitude', 'temperature', 'rpm',\n",
        "            'harmonic_distortion', 'spectral_centroid', 'zero_crossing_rate',\n",
        "            'vibration_x', 'vibration_y', 'vibration_z'\n",
        "        ]\n",
        "\n",
        "        X = data[feature_columns]\n",
        "        y = data['fault_score']\n",
        "\n",
        "        print(f\"📊 Dataset Info:\")\n",
        "        print(f\"   Total samples: {len(X)}\")\n",
        "        print(f\"   Features: {X.shape[1]}\")\n",
        "        print(f\"   Target range: {y.min():.3f} - {y.max():.3f}\")\n",
        "\n",
        "        # Split data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"   Training samples: {len(self.X_train)}\")\n",
        "        print(f\"   Testing samples: {len(self.X_test)}\")\n",
        "\n",
        "        # Scale features\n",
        "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
        "\n",
        "        # Define models with detailed parameters\n",
        "        models = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
        "            'Lasso Regression': Lasso(alpha=0.1, random_state=42),\n",
        "            'Random Forest': RandomForestRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=10,\n",
        "                random_state=42\n",
        "            ),\n",
        "            'Gradient Boosting': GradientBoostingRegressor(\n",
        "                n_estimators=100,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=6,\n",
        "                random_state=42\n",
        "            ),\n",
        "            'Support Vector Regression': SVR(\n",
        "                kernel='rbf',\n",
        "                C=1.0,\n",
        "                gamma='scale',\n",
        "                epsilon=0.1\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Train and evaluate models\n",
        "        results = {}\n",
        "\n",
        "        for name, model in models.items():\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"🔄 Training {name}...\")\n",
        "\n",
        "            # Train model\n",
        "            if name in ['Support Vector Regression', 'Ridge Regression', 'Lasso Regression']:\n",
        "                model.fit(self.X_train_scaled, self.y_train)\n",
        "                y_pred = model.predict(self.X_test_scaled)\n",
        "                y_train_pred = model.predict(self.X_train_scaled)\n",
        "            else:\n",
        "                model.fit(self.X_train, self.y_train)\n",
        "                y_pred = model.predict(self.X_test)\n",
        "                y_train_pred = model.predict(self.X_train)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mse = mean_squared_error(self.y_test, y_pred)\n",
        "            r2 = r2_score(self.y_test, y_pred)\n",
        "            mae = mean_absolute_error(self.y_test, y_pred)\n",
        "\n",
        "            # Training metrics\n",
        "            train_mse = mean_squared_error(self.y_train, y_train_pred)\n",
        "            train_r2 = r2_score(self.y_train, y_train_pred)\n",
        "\n",
        "            # Cross-validation\n",
        "            if name in ['Support Vector Regression', 'Ridge Regression', 'Lasso Regression']:\n",
        "                cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=5, scoring='r2')\n",
        "            else:\n",
        "                cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='r2')\n",
        "\n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'mse': mse,\n",
        "                'r2': r2,\n",
        "                'mae': mae,\n",
        "                'train_mse': train_mse,\n",
        "                'train_r2': train_r2,\n",
        "                'cv_mean': cv_scores.mean(),\n",
        "                'cv_std': cv_scores.std(),\n",
        "                'predictions': y_pred,\n",
        "                'train_predictions': y_train_pred\n",
        "            }\n",
        "\n",
        "            # Print model parameters\n",
        "            self.print_model_parameters(model, name)\n",
        "\n",
        "            print(f\"\\n📈 Performance Metrics:\")\n",
        "            print(f\"   Training R²: {train_r2:.4f}\")\n",
        "            print(f\"   Training MSE: {train_mse:.4f}\")\n",
        "            print(f\"   Test R²: {r2:.4f}\")\n",
        "            print(f\"   Test MSE: {mse:.4f}\")\n",
        "            print(f\"   Test MAE: {mae:.4f}\")\n",
        "            print(f\"   CV R² (mean ± std): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "            # Check for overfitting\n",
        "            if train_r2 - r2 > 0.1:\n",
        "                print(f\"   ⚠️  Potential overfitting detected!\")\n",
        "            else:\n",
        "                print(f\"   ✅ Good generalization\")\n",
        "\n",
        "        self.models = results\n",
        "        self.is_trained = True\n",
        "\n",
        "        # Find best model\n",
        "        best_model_name = max(results.keys(), key=lambda x: results[x]['r2'])\n",
        "        self.best_model = results[best_model_name]['model']\n",
        "        self.best_model_name = best_model_name\n",
        "\n",
        "        print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "        print(f\"   Best Test R²: {results[best_model_name]['r2']:.4f}\")\n",
        "        print(f\"   Best Test MSE: {results[best_model_name]['mse']:.4f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_regression_lines_and_residuals(self):\n",
        "        \"\"\"Plot regression lines and residual plots for all models\"\"\"\n",
        "        if not self.is_trained:\n",
        "            print(\"❌ Models not trained. Please train models first.\")\n",
        "            return\n",
        "\n",
        "        n_models = len(self.models)\n",
        "        fig, axes = plt.subplots(2, n_models, figsize=(4*n_models, 8))\n",
        "\n",
        "        if n_models == 1:\n",
        "            axes = axes.reshape(2, 1)\n",
        "        elif n_models > 1:\n",
        "            axes = axes.flatten().reshape(2, n_models)\n",
        "\n",
        "        fig.suptitle('Model Performance: Regression Lines and Residual Plots', fontsize=16, fontweight='bold')\n",
        "\n",
        "        for i, (name, results) in enumerate(self.models.items()):\n",
        "            # Top row: Regression line (Predicted vs Actual)\n",
        "            ax1 = axes[0, i]\n",
        "            predictions = results['predictions']\n",
        "\n",
        "            # Scatter plot\n",
        "            ax1.scatter(self.y_test, predictions, alpha=0.6, color='blue', s=30)\n",
        "\n",
        "            # Perfect prediction line\n",
        "            min_val = min(self.y_test.min(), predictions.min())\n",
        "            max_val = max(self.y_test.max(), predictions.max())\n",
        "            ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "\n",
        "            # Regression line\n",
        "            z = np.polyfit(self.y_test, predictions, 1)\n",
        "            p = np.poly1d(z)\n",
        "            ax1.plot(self.y_test, p(self.y_test), 'g-', lw=2, label=f'Regression Line')\n",
        "\n",
        "            ax1.set_xlabel('Actual Fault Score')\n",
        "            ax1.set_ylabel('Predicted Fault Score')\n",
        "            ax1.set_title(f'{name}\\nR² = {results[\"r2\"]:.4f}')\n",
        "            ax1.legend()\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # Bottom row: Residual plot\n",
        "            ax2 = axes[1, i]\n",
        "            residuals = self.y_test - predictions\n",
        "\n",
        "            # Scatter plot of residuals\n",
        "            ax2.scatter(predictions, residuals, alpha=0.6, color='red', s=30)\n",
        "\n",
        "            # Zero line\n",
        "            ax2.axhline(y=0, color='black', linestyle='--', lw=2)\n",
        "\n",
        "            # Trend line for residuals\n",
        "            z_res = np.polyfit(predictions, residuals, 1)\n",
        "            p_res = np.poly1d(z_res)\n",
        "            ax2.plot(predictions, p_res(predictions), 'g-', lw=2, alpha=0.7)\n",
        "\n",
        "            ax2.set_xlabel('Predicted Fault Score')\n",
        "            ax2.set_ylabel('Residuals')\n",
        "            ax2.set_title(f'Residual Plot\\nMSE = {results[\"mse\"]:.4f}')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def display_evaluation_metrics(self):\n",
        "        \"\"\"Display comprehensive evaluation metrics\"\"\"\n",
        "        if not self.is_trained:\n",
        "            print(\"❌ Models not trained. Please train models first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n📊 COMPREHENSIVE EVALUATION METRICS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Create DataFrame for better display\n",
        "        metrics_data = []\n",
        "        for name, results in self.models.items():\n",
        "            metrics_data.append({\n",
        "                'Model': name,\n",
        "                'Train R²': results['train_r2'],\n",
        "                'Test R²': results['r2'],\n",
        "                'Train MSE': results['train_mse'],\n",
        "                'Test MSE': results['mse'],\n",
        "                'MAE': results['mae'],\n",
        "                'CV Mean': results['cv_mean'],\n",
        "                'CV Std': results['cv_std'],\n",
        "                'Overfitting': results['train_r2'] - results['r2']\n",
        "            })\n",
        "\n",
        "        df_metrics = pd.DataFrame(metrics_data)\n",
        "\n",
        "        # Display formatted table\n",
        "        print(\"\\n📈 Model Performance Summary:\")\n",
        "        print(\"-\" * 80)\n",
        "        for _, row in df_metrics.iterrows():\n",
        "            print(f\"\\n🔧 {row['Model']}:\")\n",
        "            print(f\"   Training R²:     {row['Train R²']:.4f}\")\n",
        "            print(f\"   Testing R²:      {row['Test R²']:.4f}\")\n",
        "            print(f\"   Training MSE:    {row['Train MSE']:.4f}\")\n",
        "            print(f\"   Testing MSE:     {row['Test MSE']:.4f}\")\n",
        "            print(f\"   MAE:            {row['MAE']:.4f}\")\n",
        "            print(f\"   CV Score:       {row['CV Mean']:.4f} ± {row['CV Std']:.4f}\")\n",
        "            print(f\"   Overfitting:    {row['Overfitting']:.4f}\")\n",
        "\n",
        "            # Performance assessment\n",
        "            if row['Test R²'] > 0.8:\n",
        "                print(\"   Status: ✅ Excellent\")\n",
        "            elif row['Test R²'] > 0.6:\n",
        "                print(\"   Status: ⚠️  Good\")\n",
        "            else:\n",
        "                print(\"   Status: ❌ Needs Improvement\")\n",
        "\n",
        "        # Best model highlight\n",
        "        best_model = df_metrics.loc[df_metrics['Test R²'].idxmax()]\n",
        "        print(f\"\\n🏆 BEST MODEL: {best_model['Model']}\")\n",
        "        print(f\"   Best R²: {best_model['Test R²']:.4f}\")\n",
        "        print(f\"   Best MSE: {best_model['Test MSE']:.4f}\")\n",
        "\n",
        "        # Statistical significance\n",
        "        print(f\"\\n📊 Statistical Analysis:\")\n",
        "        print(f\"   Mean R² across models: {df_metrics['Test R²'].mean():.4f}\")\n",
        "        print(f\"   Std R² across models: {df_metrics['Test R²'].std():.4f}\")\n",
        "        print(f\"   Best vs Worst R² gap: {df_metrics['Test R²'].max() - df_metrics['Test R²'].min():.4f}\")\n",
        "\n",
        "        return df_metrics\n",
        "\n",
        "    def create_detailed_residual_analysis(self):\n",
        "        \"\"\"Create detailed residual analysis plots\"\"\"\n",
        "        if not self.is_trained:\n",
        "            print(\"❌ Models not trained. Please train models first.\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle(f'Detailed Residual Analysis - {self.best_model_name}', fontsize=16, fontweight='bold')\n",
        "\n",
        "        best_results = self.models[self.best_model_name]\n",
        "        predictions = best_results['predictions']\n",
        "        residuals = self.y_test - predictions\n",
        "\n",
        "        # 1. Residuals vs Predicted\n",
        "        axes[0, 0].scatter(predictions, residuals, alpha=0.6, color='blue')\n",
        "        axes[0, 0].axhline(y=0, color='red', linestyle='--', lw=2)\n",
        "        axes[0, 0].set_xlabel('Predicted Values')\n",
        "        axes[0, 0].set_ylabel('Residuals')\n",
        "        axes[0, 0].set_title('Residuals vs Predicted')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Histogram of residuals\n",
        "        axes[0, 1].hist(residuals, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "        axes[0, 1].axvline(x=0, color='red', linestyle='--', lw=2)\n",
        "        axes[0, 1].set_xlabel('Residuals')\n",
        "        axes[0, 1].set_ylabel('Frequency')\n",
        "        axes[0, 1].set_title('Distribution of Residuals')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Q-Q plot\n",
        "        from scipy import stats\n",
        "        stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
        "        axes[1, 0].set_title('Q-Q Plot (Normality Test)')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Residuals vs Actual\n",
        "        axes[1, 1].scatter(self.y_test, residuals, alpha=0.6, color='orange')\n",
        "        axes[1, 1].axhline(y=0, color='red', linestyle='--', lw=2)\n",
        "        axes[1, 1].set_xlabel('Actual Values')\n",
        "        axes[1, 1].set_ylabel('Residuals')\n",
        "        axes[1, 1].set_title('Residuals vs Actual')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Statistical tests\n",
        "        print(f\"\\n📊 Residual Analysis Statistics:\")\n",
        "        print(f\"   Mean of residuals: {np.mean(residuals):.6f}\")\n",
        "        print(f\"   Std of residuals: {np.std(residuals):.6f}\")\n",
        "        print(f\"   Skewness: {stats.skew(residuals):.4f}\")\n",
        "        print(f\"   Kurtosis: {stats.kurtosis(residuals):.4f}\")\n",
        "\n",
        "        # Normality test\n",
        "        shapiro_stat, shapiro_p = stats.shapiro(residuals)\n",
        "        print(f\"   Shapiro-Wilk test: W={shapiro_stat:.4f}, p={shapiro_p:.4f}\")\n",
        "        if shapiro_p > 0.05:\n",
        "            print(\"   ✅ Residuals appear normally distributed\")\n",
        "        else:\n",
        "            print(\"   ⚠️  Residuals may not be normally distributed\")\n",
        "\n",
        "    def predict_and_display_results(self, test_data=None):\n",
        "        \"\"\"Predict and display results with comprehensive analysis\"\"\"\n",
        "        if not self.is_trained:\n",
        "            print(\"❌ Models not trained. Please train models first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n🔮 PREDICTION AND RESULTS DISPLAY\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Use test data if provided, otherwise generate sample\n",
        "        if test_data is None:\n",
        "            test_data = {\n",
        "                'frequency': 2800,\n",
        "                'amplitude': 0.75,\n",
        "                'temperature': 105,\n",
        "                'rpm': 4200,\n",
        "                'harmonic_distortion': 0.08,\n",
        "                'spectral_centroid': 3200,\n",
        "                'zero_crossing_rate': 0.45,\n",
        "                'vibration_x': 0.12,\n",
        "                'vibration_y': 0.08,\n",
        "                'vibration_z': 0.15\n",
        "            }\n",
        "\n",
        "        print(f\"📊 Input Parameters:\")\n",
        "        for key, value in test_data.items():\n",
        "            print(f\"   {key}: {value}\")\n",
        "\n",
        "        # Get predictions from all models\n",
        "        print(f\"\\n🤖 Predictions from All Models:\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        feature_columns = [\n",
        "            'frequency', 'amplitude', 'temperature', 'rpm',\n",
        "            'harmonic_distortion', 'spectral_centroid', 'zero_crossing_rate',\n",
        "            'vibration_x', 'vibration_y', 'vibration_z'\n",
        "        ]\n",
        "\n",
        "        X_sample = pd.DataFrame([test_data])[feature_columns]\n",
        "\n",
        "        all_predictions = {}\n",
        "        for name, results in self.models.items():\n",
        "            model = results['model']\n",
        "\n",
        "            if name in ['Support Vector Regression', 'Ridge Regression', 'Lasso Regression']:\n",
        "                X_scaled = self.scaler.transform(X_sample)\n",
        "                prediction = model.predict(X_scaled)[0]\n",
        "            else:\n",
        "                prediction = model.predict(X_sample)[0]\n",
        "\n",
        "            # Determine status\n",
        "            if prediction < 0.3:\n",
        "                status = \"Normal\"\n",
        "                color = \"🟢\"\n",
        "            elif prediction < 0.7:\n",
        "                status = \"Warning\"\n",
        "                color = \"🟡\"\n",
        "            else:\n",
        "                status = \"Critical\"\n",
        "                color = \"🔴\"\n",
        "\n",
        "            all_predictions[name] = {\n",
        "                'score': prediction,\n",
        "                'status': status,\n",
        "                'color': color\n",
        "            }\n",
        "\n",
        "            print(f\"{color} {name:25} | Score: {prediction:.4f} | Status: {status}\")\n",
        "\n",
        "        # Best model prediction\n",
        "        best_prediction = all_predictions[self.best_model_name]\n",
        "        print(f\"\\n🏆 Best Model ({self.best_model_name}) Prediction:\")\n",
        "        print(f\"   {best_prediction['color']} Fault Score: {best_prediction['score']:.4f}\")\n",
        "        print(f\"   Status: {best_prediction['status']}\")\n",
        "\n",
        "        # Confidence analysis\n",
        "        scores = [pred['score'] for pred in all_predictions.values()]\n",
        "        mean_score = np.mean(scores)\n",
        "        std_score = np.std(scores)\n",
        "\n",
        "        print(f\"\\n📊 Prediction Confidence Analysis:\")\n",
        "        print(f\"   Mean prediction: {mean_score:.4f}\")\n",
        "        print(f\"   Standard deviation: {std_score:.4f}\")\n",
        "        print(f\"   Confidence interval: [{mean_score - 1.96*std_score:.4f}, {mean_score + 1.96*std_score:.4f}]\")\n",
        "\n",
        "        if std_score < 0.1:\n",
        "            print(\"   ✅ High confidence - Models agree well\")\n",
        "        elif std_score < 0.2:\n",
        "            print(\"   ⚠️  Medium confidence - Some model disagreement\")\n",
        "        else:\n",
        "            print(\"   ❌ Low confidence - Significant model disagreement\")\n",
        "\n",
        "        return all_predictions\n",
        "\n",
        "    def run_complete_analysis(self):\n",
        "        \"\"\"Run the complete enhanced analysis pipeline\"\"\"\n",
        "        print(\"🚀 STARTING COMPLETE ENHANCED ACOUSTIC FAULT DETECTION ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Step 1: Generate datasets\n",
        "        self.generate_datasets(100)\n",
        "\n",
        "        # Step 2: Create and train models\n",
        "        self.create_and_train_models()\n",
        "\n",
        "        # Step 3: Plot regression lines and residuals\n",
        "        self.plot_regression_lines_and_residuals()\n",
        "\n",
        "        # Step 4: Display evaluation metrics\n",
        "        metrics_df = self.display_evaluation_metrics()\n",
        "\n",
        "        # Step 5: Detailed residual analysis\n",
        "        self.create_detailed_residual_analysis()\n",
        "\n",
        "        # Step 6: Predict and display results\n",
        "        predictions = self.predict_and_display_results()\n",
        "\n",
        "        print(\"\\n🎉 COMPLETE ENHANCED ANALYSIS FINISHED!\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return {\n",
        "            'models': self.models,\n",
        "            'metrics': metrics_df,\n",
        "            'predictions': predictions,\n",
        "            'best_model': self.best_model_name\n",
        "        }\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    \"\"\"Main function to run the enhanced acoustic fault detection system\"\"\"\n",
        "\n",
        "    # Initialize enhanced system\n",
        "    detector = EnhancedAcousticFaultDetection()\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = detector.run_complete_analysis()\n",
        "\n",
        "    # Additional example predictions\n",
        "    print(\"\\n🧪 ADDITIONAL EXAMPLE PREDICTIONS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Test cases\n",
        "    test_cases = [\n",
        "        {\n",
        "            'name': 'Normal Engine',\n",
        "            'data': {\n",
        "                'frequency': 1200, 'amplitude': 0.3, 'temperature': 80, 'rpm': 2500,\n",
        "                'harmonic_distortion': 0.02, 'spectral_centroid': 1400, 'zero_crossing_rate': 0.4,\n",
        "                'vibration_x': 0.05, 'vibration_y': 0.03, 'vibration_z': 0.04\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            'name': 'Warning Engine',\n",
        "            'data': {\n",
        "                'frequency': 2200, 'amplitude': 0.6, 'temperature': 110, 'rpm': 4000,\n",
        "                'harmonic_distortion': 0.07, 'spectral_centroid': 2500, 'zero_crossing_rate': 0.3,\n",
        "                                'vibration_x': 0.08, 'vibration_y': 0.07, 'vibration_z': 0.09\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            'name': 'Critical Engine',\n",
        "            'data': {\n",
        "                'frequency': 3600, 'amplitude': 0.9, 'temperature': 130, 'rpm': 5500,\n",
        "                'harmonic_distortion': 0.12, 'spectral_centroid': 4000, 'zero_crossing_rate': 0.75,\n",
        "                'vibration_x': 0.15, 'vibration_y': 0.18, 'vibration_z': 0.2\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Run predictions on all test cases\n",
        "    for case in test_cases:\n",
        "        print(f\"\\n🔍 {case['name']}\")\n",
        "        print(\"-\" * 60)\n",
        "        detector.predict_and_display_results(case['data'])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}